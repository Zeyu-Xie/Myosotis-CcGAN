{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dfc621",
   "metadata": {},
   "source": [
    "# CcGAN Train\n",
    "\n",
    "This notebook is used to train CcGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d1100",
   "metadata": {},
   "source": [
    "## Step 1 - Import Libraries and Set Arguments\n",
    "\n",
    "First we import others' libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e679105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:16.300729Z",
     "iopub.status.busy": "2025-06-11T23:17:16.300471Z",
     "iopub.status.idle": "2025-06-11T23:17:19.249744Z",
     "shell.execute_reply": "2025-06-11T23:17:19.249229Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import timeit\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e4000",
   "metadata": {},
   "source": [
    "Next import our own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5116c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.252633Z",
     "iopub.status.busy": "2025-06-11T23:17:19.251956Z",
     "iopub.status.idle": "2025-06-11T23:17:19.256448Z",
     "shell.execute_reply": "2025-06-11T23:17:19.255998Z"
    }
   },
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5b12c",
   "metadata": {},
   "source": [
    "And then we set the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a37af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.258369Z",
     "iopub.status.busy": "2025-06-11T23:17:19.258087Z",
     "iopub.status.idle": "2025-06-11T23:17:19.262774Z",
     "shell.execute_reply": "2025-06-11T23:17:19.262352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Overall Settings\n",
    "DATA_PATH = \"/home/ubuntu/Desktop/Ra/Ra_128_indexed_binned.h5\"\n",
    "SAVE_OUTPUTS_DIR = \"./output/saved_outputs\"\n",
    "SAVE_IMAGES_DIR = \"./output/saved_images\"\n",
    "EMBED_MODELS_DIR = \"./output/embed_models\"\n",
    "CCGAN_MODELS_DIR = \"./output/CcGAN_models\"\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Dataset\n",
    "DATA_SPLIT = \"train\"\n",
    "MIN_LABEL = 1.5\n",
    "MAX_LABEL = 4.9\n",
    "NUM_CHANNELS = 3\n",
    "IMG_SIZE = 128\n",
    "MAX_NUM_IMG_PER_LABEL = 25\n",
    "MAX_NUM_IMG_PER_LABEL_AFTER_REPLICA = 0\n",
    "SHOW_REAL_IMGS = True\n",
    "VISUALIZE_FAKE_IMAGES = True\n",
    "\n",
    "# Embedding Settings\n",
    "BASE_LR_X2Y = 0.01\n",
    "BASE_LR_Y2H = 0.01\n",
    "\n",
    "# GAN Settings\n",
    "GAN = \"CcGAN\"\n",
    "GAN_ARCH = \"SAGAN\"\n",
    "NET_EMBED = \"ResNet34_embed\"\n",
    "EPOCH_CNN_EMBED = 200\n",
    "RESUMEEPOCH_CNN_EMBED = 0\n",
    "EPOCH_NET_Y2H = 500\n",
    "DIM_EMBED = 128\n",
    "BATCH_SIZE_EMBED = 256\n",
    "\n",
    "LOSS_TYPE_GAN = \"hinge\"\n",
    "NITERS_GAN = 30000\n",
    "RESUME_NITERS_GAN = 0\n",
    "SAVE_NITERS_FREQ = 5000\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "DIM_GAN = 128\n",
    "BATCH_SIZE_DISC = 64\n",
    "BATCH_SIZE_GENE = 64\n",
    "NUM_D_STEPS = 2\n",
    "CGAN_NUM_CLASSES = 20\n",
    "VISUALIZE_FREQ = 1000\n",
    "\n",
    "KERNEL_SIGMA = -1.0\n",
    "THRESHOLD_TYPE = \"soft\"\n",
    "KAPPA = -2.0\n",
    "NONZERO_SOFT_WEIGHT_THRESHOLD = 1e-3\n",
    "\n",
    "# DiffAugment Settings\n",
    "GAN_DIFFAUGMENT = True\n",
    "GAN_DIFFAUGMENT_POLICY = \"color,translation,cutout\"\n",
    "\n",
    "# Evaluation Settings\n",
    "EVAL_MODE = 2\n",
    "NUM_EVAL_LABELS = -1\n",
    "SAMP_BATCH_SIZE = 1000\n",
    "NFAKE_PER_LABEL = 200\n",
    "NREAL_PER_LABEL = -1\n",
    "COMP_FID = True\n",
    "EPOCH_FID_CNN = 200\n",
    "FID_RADIUS = 0\n",
    "FID_NUM_CENTERS = -1\n",
    "DUMP_FAKE_FOR_NIQE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b283b",
   "metadata": {},
   "source": [
    "## Step 2 - Define Basic Classes and Functions\n",
    "\n",
    "We need to use some basic classes and functions later, so we define them here.\n",
    "\n",
    "### Step 2.1 - Function `normalize_images`\n",
    "\n",
    "Normalize the batch images' pixel values from $[0, 255]$ to $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadc119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.264452Z",
     "iopub.status.busy": "2025-06-11T23:17:19.264287Z",
     "iopub.status.idle": "2025-06-11T23:17:19.266700Z",
     "shell.execute_reply": "2025-06-11T23:17:19.266296Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_images(batch_images):\n",
    "    batch_images = batch_images / 255.0\n",
    "    batch_images = (batch_images - 0.5) / 0.5\n",
    "    return batch_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde855b4",
   "metadata": {},
   "source": [
    "### Step 2.2 - Function `view_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9886cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.268580Z",
     "iopub.status.busy": "2025-06-11T23:17:19.268427Z",
     "iopub.status.idle": "2025-06-11T23:17:19.271279Z",
     "shell.execute_reply": "2025-06-11T23:17:19.270848Z"
    }
   },
   "outputs": [],
   "source": [
    "def _print_hdf5(name, obj):\n",
    "    indent = \"  \" * name.count(\"/\")\n",
    "    if isinstance(obj, h5py.Dataset):\n",
    "        print(f\"{indent}[Dataset] {name} shape={obj.shape} dtype={obj.dtype}\")\n",
    "    elif isinstance(obj, h5py.Group):\n",
    "        print(f\"{indent}[Group]   {name}\")\n",
    "\n",
    "\n",
    "def view_dataset(dataset_path):\n",
    "    with h5py.File(dataset_path, \"r\") as f:\n",
    "        f.visititems(_print_hdf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afda88",
   "metadata": {},
   "source": [
    "### Step 2.3 - Class `Label_dataset`\n",
    "\n",
    "`Imgs_dataset` is derived from class `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd85208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.273040Z",
     "iopub.status.busy": "2025-06-11T23:17:19.272853Z",
     "iopub.status.idle": "2025-06-11T23:17:19.275688Z",
     "shell.execute_reply": "2025-06-11T23:17:19.275271Z"
    }
   },
   "outputs": [],
   "source": [
    "class Label_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, labels):\n",
    "        super(Label_dataset, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.n_samples = len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        y = self.labels[index]\n",
    "        return y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380a083",
   "metadata": {},
   "source": [
    "### Step 2.4 - Class `Imgs_dataset`\n",
    "\n",
    "`Imgs_dataset` is derived from class `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ee876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.277508Z",
     "iopub.status.busy": "2025-06-11T23:17:19.277263Z",
     "iopub.status.idle": "2025-06-11T23:17:19.280989Z",
     "shell.execute_reply": "2025-06-11T23:17:19.280573Z"
    }
   },
   "outputs": [],
   "source": [
    "class Imgs_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels=None, normalize=False):\n",
    "        super(Imgs_dataset, self).__init__()\n",
    "        self.images = images\n",
    "        self.n_images = len(self.images)\n",
    "        self.labels = labels\n",
    "        if labels is not None:\n",
    "            if len(self.images) != len(self.labels):\n",
    "                raise Exception(\n",
    "                    \"images (\"\n",
    "                    + str(len(self.images))\n",
    "                    + \") and labels (\"\n",
    "                    + str(len(self.labels))\n",
    "                    + \") do not have the same length!!!\"\n",
    "                )\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        if self.normalize:\n",
    "            image = image / 255.0\n",
    "            image = (image - 0.5) / 0.5\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return (image, label)\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d911c91",
   "metadata": {},
   "source": [
    "## Step 3 - Settings\n",
    "\n",
    "In this part we would like to make configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc8eeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.282778Z",
     "iopub.status.busy": "2025-06-11T23:17:19.282528Z",
     "iopub.status.idle": "2025-06-11T23:17:19.287061Z",
     "shell.execute_reply": "2025-06-11T23:17:19.286646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seeds\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "os.makedirs(SAVE_OUTPUTS_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(EMBED_MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(CCGAN_MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e5f62",
   "metadata": {},
   "source": [
    "## Step 4 - Load and Process Data\n",
    "\n",
    "In the next code block we will get the following `numpy.ndarray`: \n",
    "\n",
    "1. `labels_all`: all the original labels\n",
    "2. `images_all`: all the original images, shape `(N, 3, H, W)`\n",
    "3. `index_train`: indexes for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0890e8c",
   "metadata": {},
   "source": [
    "### Step 4.1 - Load data from the `.h5` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564e047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.288901Z",
     "iopub.status.busy": "2025-06-11T23:17:19.288708Z",
     "iopub.status.idle": "2025-06-11T23:17:19.304754Z",
     "shell.execute_reply": "2025-06-11T23:17:19.304300Z"
    }
   },
   "outputs": [],
   "source": [
    "# View dataset structure\n",
    "view_dataset(DATA_PATH)\n",
    "print(\"\")\n",
    "\n",
    "# Load data from h5 file\n",
    "hf = h5py.File(DATA_PATH, 'r')\n",
    "labels_all = hf['labels'][:]\n",
    "images_all = hf['images'][:]\n",
    "index_train = hf['index_train'][:]\n",
    "hf.close()\n",
    "\n",
    "# Change the data type and shape to fit the model\n",
    "labels_all = labels_all.astype(float)\n",
    "images_all = images_all.transpose(0, 3, 1, 2)\n",
    "\n",
    "print(f\"`images_all` shape: {images_all.shape}, dtype: {images_all.dtype}\")\n",
    "print(f\"`labels_all` shape: {labels_all.shape}, dtype: {labels_all.dtype}\")\n",
    "print(f\"`index_train` shape: {index_train.shape}, dtype: {index_train.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cea6c9",
   "metadata": {},
   "source": [
    "### Step 4.2 - Split the training dataset\n",
    "\n",
    "We split the data. Rule:\n",
    "\n",
    "- If `args.data_split` is `train`, use `index_train` as the set of training indexes.\n",
    "- Otherwise, use all data as the training set.\n",
    "\n",
    "After the next block we will get:\n",
    "\n",
    "1. `images_train`: the training set images (subset of `images_all`)\n",
    "2. `labels_train_raw`: the training set labels (subset of `labels_all`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf0f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.306699Z",
     "iopub.status.busy": "2025-06-11T23:17:19.306444Z",
     "iopub.status.idle": "2025-06-11T23:17:19.311578Z",
     "shell.execute_reply": "2025-06-11T23:17:19.311132Z"
    }
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "print(f\"Data split: {DATA_SPLIT}\")\n",
    "print(\"\")\n",
    "if DATA_SPLIT == \"train\":\n",
    "    images_train = images_all[index_train]\n",
    "    labels_train_raw = labels_all[index_train]\n",
    "else:\n",
    "    images_train = copy.deepcopy(images_all)\n",
    "    labels_train_raw = copy.deepcopy(labels_all)\n",
    "\n",
    "# Print the data split result\n",
    "print(f\"`images_train` shape: {images_train.shape}, dtype: {images_train.dtype}\")\n",
    "print(f\"`labels_train_raw` shape: {labels_train_raw.shape}, dtype: {labels_train_raw.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac452e",
   "metadata": {},
   "source": [
    "### Step 4.3 - Label range limitation\n",
    "\n",
    "We only take images with label in $\\left(q_1, q_2\\right)$, where $q_1 = \\text{min label}$ and $q_2 = \\text{max label}$.\n",
    "\n",
    "In the following block we set limitation on training data. And if `args.visualize_fake_images` is `True` or `args.comp_FID` is `True`, we will also set limitation on all data.\n",
    "\n",
    "In this part we updated `images_train` and `labels_train_raw` (training data), as well as `images_all` and `labels_all` (all data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb4f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.313550Z",
     "iopub.status.busy": "2025-06-11T23:17:19.313300Z",
     "iopub.status.idle": "2025-06-11T23:17:19.324598Z",
     "shell.execute_reply": "2025-06-11T23:17:19.324134Z"
    }
   },
   "outputs": [],
   "source": [
    "# Limitation on training data\n",
    "q1 = MIN_LABEL\n",
    "q2 = MAX_LABEL\n",
    "indx = np.where((labels_train_raw>q1)*(labels_train_raw<q2)==True)[0]\n",
    "labels_train_raw = labels_train_raw[indx]\n",
    "images_train = images_train[indx]\n",
    "assert len(labels_train_raw)==len(images_train)\n",
    "print(f\"`images_train` shape: {images_train.shape}, dtype: {images_train.dtype}\")\n",
    "print(f\"`labels_train_raw` shape: {labels_train_raw.shape}, dtype: {labels_train_raw.dtype}\")\n",
    "print(\"\")\n",
    "\n",
    "# Limitation on all data\n",
    "if VISUALIZE_FAKE_IMAGES or COMP_FID:\n",
    "    indx = np.where((labels_all>q1)*(labels_all<q2)==True)[0]\n",
    "    labels_all = labels_all[indx]\n",
    "    images_all = images_all[indx]\n",
    "    assert len(labels_all)==len(images_all)\n",
    "    print(f\"`images_all` shape: {images_all.shape}, dtype: {images_all.dtype}\")\n",
    "    print(f\"`labels_all` shape: {labels_all.shape}, dtype: {labels_all.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77b06e",
   "metadata": {},
   "source": [
    "### Step 4.4 - Image number limitation\n",
    "\n",
    "For each label value, we allow at most `args.max_num_img_per_label` images occur in the training set. This step randomly selects the images and labels and update `images_train` and `labels_train_raw`.\n",
    "\n",
    "This step may only have influences on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641157f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.326574Z",
     "iopub.status.busy": "2025-06-11T23:17:19.326314Z",
     "iopub.status.idle": "2025-06-11T23:17:19.336562Z",
     "shell.execute_reply": "2025-06-11T23:17:19.336077Z"
    }
   },
   "outputs": [],
   "source": [
    "# for each angle, take no more than args.max_num_img_per_label images\n",
    "image_num_threshold = MAX_NUM_IMG_PER_LABEL\n",
    "print(\"Original set has {} images; For each angle, take no more than {} images.\".format(len(images_train), image_num_threshold))\n",
    "unique_labels_tmp = np.sort(np.array(list(set(labels_train_raw))))\n",
    "for i in tqdm(range(len(unique_labels_tmp))):\n",
    "    indx_i = np.where(labels_train_raw == unique_labels_tmp[i])[0]\n",
    "    if len(indx_i)>image_num_threshold:\n",
    "        np.random.shuffle(indx_i)\n",
    "        indx_i = indx_i[0:image_num_threshold]\n",
    "    if i == 0:\n",
    "        sel_indx = indx_i\n",
    "    else:\n",
    "        sel_indx = np.concatenate((sel_indx, indx_i))\n",
    "images_train = images_train[sel_indx]\n",
    "labels_train_raw = labels_train_raw[sel_indx]\n",
    "print(\"{} images left and there are {} unique labels\".format(len(images_train), len(set(labels_train_raw))))\n",
    "print(\"\")\n",
    "print(f\"`images_train` shape: {images_train.shape}, dtype: {images_train.dtype}\")\n",
    "print(f\"`labels_train_raw` shape: {labels_train_raw.shape}, dtype: {labels_train_raw.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fe402",
   "metadata": {},
   "source": [
    "### Step 4.5 - Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eefcc8",
   "metadata": {},
   "source": [
    "From the following block we get `labels_train` — the normalized labels, ranging from $0$ to $1$.\n",
    "\n",
    "$$\n",
    "\\text{normalized training labels} = \\frac{\\text{training labels} - \\text{min label}}{\\text{max label} - \\text{min label}} \\in \\left[0, 1\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f7412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.338589Z",
     "iopub.status.busy": "2025-06-11T23:17:19.338339Z",
     "iopub.status.idle": "2025-06-11T23:17:19.341636Z",
     "shell.execute_reply": "2025-06-11T23:17:19.341228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize labels\n",
    "labels_train = (labels_train_raw - MIN_LABEL) / (MAX_LABEL - MIN_LABEL)    \n",
    "print(\"Preset `min_label`: {}, `max_label`: {}\".format(MIN_LABEL, MAX_LABEL))\n",
    "print(\"Range of `labels_train_raw`: ({},{})\".format(np.min(labels_train_raw), np.max(labels_train_raw)))\n",
    "print(\"Range of `labels_train`: ({},{})\".format(np.min(labels_train), np.max(labels_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf670dd",
   "metadata": {},
   "source": [
    "### Step 4.6 - Calculate $\\sigma$ and $\\kappa$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f04b6",
   "metadata": {},
   "source": [
    "We calculate $\\sigma$ and $\\kappa$ value of the dataset (if `KERNEL_SIGMA` or `KERNEL_KAPPA` is negative).\n",
    "\n",
    "We calculate $\\sigma$ by\n",
    "\n",
    "$$\n",
    "\\sigma = \\left(\\frac{4 \\hat{\\sigma}_{y^r}^5}{3 N^r}\\right)^{\\frac{1}{5}}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\sigma}_{y^r}$ is the sample standard deviation of normalized labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b3f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.343414Z",
     "iopub.status.busy": "2025-06-11T23:17:19.343169Z",
     "iopub.status.idle": "2025-06-11T23:17:19.347080Z",
     "shell.execute_reply": "2025-06-11T23:17:19.346666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set `kernel_sigma`\n",
    "if KERNEL_SIGMA < 0:\n",
    "\n",
    "    std_label = np.std(labels_train)\n",
    "    KERNEL_SIGMA = 1.06 * std_label * (len(labels_train)) ** (-1 / 5)\n",
    "\n",
    "print(\"`kernel_sigma`: {}\".format(KERNEL_SIGMA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1426bf",
   "metadata": {},
   "source": [
    "We calculate $\\kappa$ as follows: Let\n",
    "$$\n",
    "\\kappa_{\\text{base}} = \\max \\left(y_{[2]}^r - y_{[1]}^r, y_{[3]}^r - y_{[2]}^r, \\cdots, y_{\\left[N_{\\text{uy}}^r\\right]}^r - y_{\\left[N_{\\text{uy}}^r - 1\\right]}^r\\right)\n",
    "$$\n",
    "where $y_{[l]}^r$ is the $l$-th smallest normalized distinct real label and $N_{\\text{uy}}^r$ is the number of normalized distinct labels in the training set.\n",
    "\n",
    "Then $\\kappa$ is set as\n",
    "$$\n",
    "\\kappa = m_{\\kappa} \\kappa_{\\text{base}}\n",
    "$$\n",
    "where $m_{\\kappa}$ stands for $50\\%$ of the minimum number of neighboring labels used for estimating $p_r\\left(x | y\\right)$ given a label $y$.\n",
    "\n",
    "For example, $m_{\\kappa} = 1$ implies using $2$ neighboring labels (one on the left and the other one on the right).\n",
    "\n",
    "In experiments $m_{\\kappa}$ is generally set as $1$ or $2$. In some extreme cases when many distinct labels have too few real samples, we may consider increasing $m_{\\kappa}$. We also found $\\nu = \\frac{1}{\\kappa^2}$ works well in the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `kappa`\n",
    "if KAPPA < 0:\n",
    "    unique_labels_norm = np.sort(np.array(list(set(labels_train))))\n",
    "    n_unique = len(unique_labels_norm)\n",
    "\n",
    "    diff_list = []\n",
    "    for i in range(1, n_unique):\n",
    "        diff_list.append(unique_labels_norm[i] - unique_labels_norm[i - 1])\n",
    "    kappa_base = np.abs(KAPPA) * np.max(np.array(diff_list))\n",
    "\n",
    "    if THRESHOLD_TYPE == \"hard\":\n",
    "        KAPPA = kappa_base\n",
    "    else:\n",
    "        KAPPA = 1 / kappa_base**2\n",
    "\n",
    "print(\"`kappa`: {}\".format(KAPPA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e2965",
   "metadata": {},
   "source": [
    "## Step 5 - Pre-trained CNN and GAN for Label Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d2bd86",
   "metadata": {},
   "source": [
    "Define function `train_net_embed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb854d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.348912Z",
     "iopub.status.busy": "2025-06-11T23:17:19.348665Z",
     "iopub.status.idle": "2025-06-11T23:17:19.356428Z",
     "shell.execute_reply": "2025-06-11T23:17:19.355998Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_net_embed(\n",
    "    net,\n",
    "    net_name,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    epochs=200,\n",
    "    resume_epoch=0,\n",
    "    lr_base=0.01,\n",
    "    lr_decay_factor=0.1,\n",
    "    lr_decay_epochs=[80, 140],\n",
    "    weight_decay=1e-4,\n",
    "    path_to_ckpt=None,\n",
    "):\n",
    "\n",
    "    # Learning rate decay\n",
    "    def adjust_learning_rate_1(optimizer, epoch):\n",
    "        lr = lr_base\n",
    "        num_decays = len(lr_decay_epochs)\n",
    "        for decay_i in range(num_decays):\n",
    "            if epoch >= lr_decay_epochs[decay_i]:\n",
    "                lr = lr * lr_decay_factor\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    # Set up the model\n",
    "    net = net.cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        net.parameters(), lr=lr_base, momentum=0.9, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Load checkpoint if `resume_epoch` > 0\n",
    "    if path_to_ckpt is not None and resume_epoch > 0:\n",
    "        save_file = os.path.join(path_to_ckpt, \"ckpt_embed_x2y\", \"ckpt_embed_x2y_epoch_{}.pth\".format(resume_epoch))\n",
    "        checkpoint = torch.load(save_file)\n",
    "        net.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        torch.set_rng_state(checkpoint[\"rng_state\"])\n",
    "\n",
    "    # Start the timer\n",
    "    start_tmp = timeit.default_timer()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(resume_epoch, epochs):\n",
    "\n",
    "        # Train mode\n",
    "        net.train()\n",
    "\n",
    "        # Init the loss\n",
    "        train_loss = 0\n",
    "\n",
    "        # Adjust learning rate\n",
    "        adjust_learning_rate_1(optimizer, epoch)\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for _, (batch_train_images, batch_train_labels) in enumerate(trainloader):\n",
    "\n",
    "            batch_train_images = batch_train_images.type(torch.float).cuda()\n",
    "            batch_train_labels = (\n",
    "                batch_train_labels.type(torch.float).reshape(-1, 1).cuda()\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _ = net(batch_train_images)\n",
    "            loss = criterion(outputs, batch_train_labels)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the loss\n",
    "            train_loss += loss.cpu().item() * batch_train_images.size(0)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        train_loss = train_loss / len(trainloader.dataset)\n",
    "\n",
    "        # Print the training loss\n",
    "        if testloader is None:\n",
    "            print(\n",
    "                \"Train net_x2y for embedding: [epoch %d/%d] train_loss:%f Time:%.4f\"\n",
    "                % (epoch + 1, epochs, train_loss, timeit.default_timer() - start_tmp)\n",
    "            )\n",
    "        else:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                test_loss = 0\n",
    "                for batch_test_images, batch_test_labels in testloader:\n",
    "                    batch_test_images = batch_test_images.type(torch.float).cuda()\n",
    "                    batch_test_labels = (\n",
    "                        batch_test_labels.type(torch.float).reshape(-1, 1).cuda()\n",
    "                    )\n",
    "                    outputs, _ = net(batch_test_images)\n",
    "                    loss = criterion(outputs, batch_test_labels)\n",
    "                    test_loss += loss.cpu().item()\n",
    "                test_loss = test_loss / len(testloader)\n",
    "\n",
    "                print(\n",
    "                    \"Train net_x2y for label embedding: [epoch %d/%d] train_loss:%f test_loss:%f Time:%.4f\"\n",
    "                    % (\n",
    "                        epoch + 1,\n",
    "                        epochs,\n",
    "                        train_loss,\n",
    "                        test_loss,\n",
    "                        timeit.default_timer() - start_tmp,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Save checkpoint\n",
    "        if path_to_ckpt is not None and (\n",
    "            ((epoch + 1) % 50 == 0) or (epoch + 1 == epochs)\n",
    "        ):\n",
    "            save_file = os.path.join(path_to_ckpt, \"ckpt_embed_x2y\", \"ckpt_embed_x2y_epoch_{}.pth\".format(epoch + 1))\n",
    "            os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"net_state_dict\": net.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"rng_state\": torch.get_rng_state(),\n",
    "                },\n",
    "                save_file,\n",
    "            )\n",
    "\n",
    "    # Return the trained model\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d67c7",
   "metadata": {},
   "source": [
    "Define function `train_net_y2h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623333ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.358189Z",
     "iopub.status.busy": "2025-06-11T23:17:19.357931Z",
     "iopub.status.idle": "2025-06-11T23:17:19.363531Z",
     "shell.execute_reply": "2025-06-11T23:17:19.363076Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_net_y2h(\n",
    "    unique_labels_norm,\n",
    "    net_y2h,\n",
    "    net_embed,\n",
    "    epochs=500,\n",
    "    lr_base=0.01,\n",
    "    lr_decay_factor=0.1,\n",
    "    lr_decay_epochs=[150, 250, 350],\n",
    "    weight_decay=1e-4,\n",
    "    batch_size=128,\n",
    "):\n",
    "\n",
    "    # Learning rate decay function\n",
    "    def adjust_learning_rate_2(optimizer, epoch):\n",
    "        lr = lr_base\n",
    "        num_decays = len(lr_decay_epochs)\n",
    "        for decay_i in range(num_decays):\n",
    "            if epoch >= lr_decay_epochs[decay_i]:\n",
    "                lr = lr * lr_decay_factor\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    assert np.max(unique_labels_norm) <= 1 and np.min(unique_labels_norm) >= 0\n",
    "    trainset = Label_dataset(unique_labels_norm)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    net_embed.eval()\n",
    "    net_h2y = net_embed.module.h2y  # convert embedding labels to original labels\n",
    "    optimizer_y2h = torch.optim.SGD(\n",
    "        net_y2h.parameters(), lr=lr_base, momentum=0.9, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Set timer\n",
    "    start_tmp = timeit.default_timer()\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Train mode\n",
    "        net_y2h.train()\n",
    "\n",
    "        # Init the loss\n",
    "        train_loss = 0\n",
    "\n",
    "        # Adjust learning rate\n",
    "        adjust_learning_rate_2(optimizer_y2h, epoch)\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for _, batch_labels in enumerate(trainloader):\n",
    "\n",
    "            batch_labels = batch_labels.type(torch.float).reshape(-1, 1).cuda()\n",
    "\n",
    "            # generate noises which will be added to labels\n",
    "            batch_size_curr = len(batch_labels)\n",
    "            batch_gamma = np.random.normal(0, 0.2, batch_size_curr)\n",
    "            batch_gamma = (\n",
    "                torch.from_numpy(batch_gamma).reshape(-1, 1).type(torch.float).cuda()\n",
    "            )\n",
    "\n",
    "            # add noise to labels\n",
    "            batch_labels_noise = torch.clamp(batch_labels + batch_gamma, 0.0, 1.0)\n",
    "\n",
    "            # Forward pass\n",
    "            batch_hiddens_noise = net_y2h(batch_labels_noise)\n",
    "            batch_rec_labels_noise = net_h2y(batch_hiddens_noise)\n",
    "\n",
    "            loss = nn.MSELoss()(batch_rec_labels_noise, batch_labels_noise)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer_y2h.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_y2h.step()\n",
    "\n",
    "            # Update the loss\n",
    "            train_loss += loss.cpu().item() * batch_size_curr\n",
    "\n",
    "        # Calculate the average loss\n",
    "        train_loss = train_loss / len(trainloader.dataset)\n",
    "\n",
    "        print(\n",
    "            \"Train net_y2h: [epoch %d/%d] train_loss:%f Time:%.4f\"\n",
    "            % (epoch + 1, epochs, train_loss, timeit.default_timer() - start_tmp)\n",
    "        )\n",
    "\n",
    "    return net_y2h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5a6ad",
   "metadata": {},
   "source": [
    "Use the two functions to train `net_embed` and `net_y2h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b535f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:19.365343Z",
     "iopub.status.busy": "2025-06-11T23:17:19.365073Z",
     "iopub.status.idle": "2025-06-11T23:17:57.448790Z",
     "shell.execute_reply": "2025-06-11T23:17:57.448313Z"
    }
   },
   "outputs": [],
   "source": [
    "net_embed_filename_ckpt = os.path.join(\n",
    "    EMBED_MODELS_DIR,\n",
    "    \"embed_x2y_epoch_{}_seed_{}.pth\".format(EPOCH_CNN_EMBED, SEED),\n",
    ")\n",
    "net_y2h_filename_ckpt = os.path.join(\n",
    "    EMBED_MODELS_DIR, \"y2h_epoch_{}_seed_{}.pth\".format(EPOCH_NET_Y2H, SEED)\n",
    ")\n",
    "\n",
    "# Prepare the dataset for training\n",
    "trainset = Imgs_dataset(images_train, labels_train, normalize=True)\n",
    "\n",
    "# Create the DataLoader for training\n",
    "trainloader_embed_net = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE_EMBED, shuffle=True, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "# Get net embed type\n",
    "if NET_EMBED == \"ResNet18_embed\":\n",
    "    net_embed = ResNet18_embed(dim_embed=DIM_EMBED)\n",
    "elif NET_EMBED == \"ResNet34_embed\":\n",
    "    net_embed = ResNet34_embed(dim_embed=DIM_EMBED)\n",
    "elif NET_EMBED == \"ResNet50_embed\":\n",
    "    net_embed = ResNet50_embed(dim_embed=DIM_EMBED)\n",
    "net_embed = net_embed.cuda()\n",
    "net_embed = nn.DataParallel(net_embed)\n",
    "\n",
    "# Get net y2h type\n",
    "net_y2h = model_y2h(dim_embed=DIM_EMBED)\n",
    "net_y2h = net_y2h.cuda()\n",
    "net_y2h = nn.DataParallel(net_y2h)\n",
    "\n",
    "# Train net_embed first: x2h+h2y\n",
    "if not os.path.isfile(net_embed_filename_ckpt):\n",
    "    print(\"Start training CNN for label embedding.\")\n",
    "    net_embed = train_net_embed(\n",
    "        net=net_embed,\n",
    "        net_name=NET_EMBED,\n",
    "        trainloader=trainloader_embed_net,\n",
    "        testloader=None,\n",
    "        epochs=EPOCH_CNN_EMBED,\n",
    "        resume_epoch=RESUMEEPOCH_CNN_EMBED,\n",
    "        lr_base=BASE_LR_X2Y,\n",
    "        lr_decay_factor=0.1,\n",
    "        lr_decay_epochs=[80, 140],\n",
    "        weight_decay=1e-4,\n",
    "        path_to_ckpt=EMBED_MODELS_DIR,\n",
    "    )\n",
    "    # save model\n",
    "    torch.save(\n",
    "        {\n",
    "            \"net_state_dict\": net_embed.state_dict(),\n",
    "        },\n",
    "        net_embed_filename_ckpt,\n",
    "    )\n",
    "else:\n",
    "    print(\"`net_embed` ckpt already exists, loading it.\")\n",
    "    print(\"\")\n",
    "    checkpoint = torch.load(net_embed_filename_ckpt)\n",
    "    net_embed.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "\n",
    "# Train y2h\n",
    "if not os.path.isfile(net_y2h_filename_ckpt):\n",
    "    print(\"Start training `net_y2h`.\")\n",
    "    net_y2h = train_net_y2h(\n",
    "        unique_labels_norm,\n",
    "        net_y2h,\n",
    "        net_embed,\n",
    "        epochs=EPOCH_NET_Y2H,\n",
    "        lr_base=BASE_LR_Y2H,\n",
    "        lr_decay_factor=0.1,\n",
    "        lr_decay_epochs=[150, 250, 350],\n",
    "        weight_decay=1e-4,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    # save model\n",
    "    torch.save(\n",
    "        {\n",
    "            \"net_state_dict\": net_y2h.state_dict(),\n",
    "        },\n",
    "        net_y2h_filename_ckpt,\n",
    "    )\n",
    "else:\n",
    "    print(\"`net_y2h` ckpt already exists, loading it.\")\n",
    "    print(\"\")\n",
    "    checkpoint = torch.load(net_y2h_filename_ckpt)\n",
    "    net_y2h.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Net Embed Summary\")\n",
    "summary(net_embed, input_size=(NUM_CHANNELS, IMG_SIZE, IMG_SIZE))\n",
    "print(\"\")\n",
    "print(\"Net Y2H Summary\")\n",
    "summary(net_y2h, input_size=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f0152",
   "metadata": {},
   "source": [
    "Now that we have `net_embed` and `net_y2h`, we will do some simple tests on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92989c2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:57.450794Z",
     "iopub.status.busy": "2025-06-11T23:17:57.450517Z",
     "iopub.status.idle": "2025-06-11T23:17:57.817189Z",
     "shell.execute_reply": "2025-06-11T23:17:57.816656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get `indx_tmp`\n",
    "indx_tmp = np.arange(len(unique_labels_norm))\n",
    "np.random.shuffle(indx_tmp)\n",
    "indx_tmp = indx_tmp[:10]\n",
    "\n",
    "# Get `labels_tmp`\n",
    "labels_tmp = unique_labels_norm[indx_tmp].reshape(-1, 1)\n",
    "labels_tmp = torch.from_numpy(labels_tmp).type(torch.float).cuda()\n",
    "epsilons_tmp = np.random.normal(0, 0.2, len(labels_tmp))\n",
    "epsilons_tmp = torch.from_numpy(epsilons_tmp).view(-1, 1).type(torch.float).cuda()\n",
    "labels_tmp = torch.clamp(labels_tmp + epsilons_tmp, 0.0, 1.0)\n",
    "\n",
    "# Eval mode\n",
    "net_embed.eval()\n",
    "net_h2y = net_embed.module.h2y\n",
    "net_y2h.eval()\n",
    "with torch.no_grad():\n",
    "    labels_rec_tmp = net_h2y(net_y2h(labels_tmp)).cpu().numpy().reshape(-1, 1)\n",
    "results = np.concatenate((labels_tmp.cpu().numpy(), labels_rec_tmp), axis=1)\n",
    "\n",
    "# Print results\n",
    "results_df = pd.DataFrame(results, columns=[\"labels\", \"reconstructed_labels\"])\n",
    "print(results_df)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"labels\"], label=\"Original Labels\", marker=\"o\")\n",
    "plt.plot(results_df[\"reconstructed_labels\"], label=\"Reconstructed Labels\", marker=\"x\")\n",
    "plt.title(\"Original vs Reconstructed Labels\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Label Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Finish tests\n",
    "net_embed = net_embed.cpu()\n",
    "net_h2y = net_h2y.cpu()\n",
    "del net_embed, net_h2y\n",
    "gc.collect()\n",
    "net_y2h = net_y2h.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d59c7",
   "metadata": {},
   "source": [
    "## Step 6 - GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40253d",
   "metadata": {},
   "source": [
    "Define `train_ccgan`. This is a long function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515effe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:57.819698Z",
     "iopub.status.busy": "2025-06-11T23:17:57.819423Z",
     "iopub.status.idle": "2025-06-11T23:17:57.837129Z",
     "shell.execute_reply": "2025-06-11T23:17:57.836678Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_ccgan(\n",
    "    kernel_sigma,\n",
    "    kappa,\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    netG,\n",
    "    netD,\n",
    "    net_y2h,\n",
    "    SAVE_IMAGES_DIR,\n",
    "    CCGAN_MODELS_DIR=None,\n",
    "    clip_label=False,\n",
    "):\n",
    "\n",
    "    # Define loss dataframe\n",
    "    loss_df = pd.DataFrame(\n",
    "        columns=[\"niter\", \"d_loss\", \"g_loss\", \"real_prob\", \"fake_prob\"]\n",
    "    )\n",
    "\n",
    "    # Nets\n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "    net_y2h = net_y2h.cuda()\n",
    "    net_y2h.eval()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizerG = torch.optim.Adam(netG.parameters(), lr=LR_G, betas=(0.5, 0.999))\n",
    "    optimizerD = torch.optim.Adam(netD.parameters(), lr=LR_D, betas=(0.5, 0.999))\n",
    "\n",
    "    # Resume training if needed\n",
    "    if CCGAN_MODELS_DIR is not None and RESUME_NITERS_GAN > 0:\n",
    "        save_file = os.path.join(\n",
    "            CCGAN_MODELS_DIR,\n",
    "            \"CcGAN_{}_nDsteps_{}\".format(THRESHOLD_TYPE, NUM_D_STEPS),\n",
    "            \"ckpt_CcGAN_niters_{}.pth\".format(RESUME_NITERS_GAN),\n",
    "        )\n",
    "        checkpoint = torch.load(save_file)\n",
    "        netG.load_state_dict(checkpoint[\"netG_state_dict\"])\n",
    "        netD.load_state_dict(checkpoint[\"netD_state_dict\"])\n",
    "        optimizerG.load_state_dict(checkpoint[\"optimizerG_state_dict\"])\n",
    "        optimizerD.load_state_dict(checkpoint[\"optimizerD_state_dict\"])\n",
    "        torch.set_rng_state(checkpoint[\"rng_state\"])\n",
    "        print(\"Resuming training from {} iterations.\".format(RESUME_NITERS_GAN))\n",
    "    else:\n",
    "        print(\"Training from scratch, no resume.\")\n",
    "\n",
    "    unique_train_labels = np.sort(np.array(list(set(train_labels))))\n",
    "\n",
    "    # Output parameters\n",
    "    n_row = 10\n",
    "    n_col = 10\n",
    "\n",
    "    z_fixed = torch.randn(n_row * n_col, DIM_GAN, dtype=torch.float).cuda()\n",
    "\n",
    "    start_label = np.quantile(train_labels, 0.05)\n",
    "    end_label = np.quantile(train_labels, 0.95)\n",
    "    selected_labels = np.linspace(start_label, end_label, num=n_row)\n",
    "\n",
    "    y_fixed = np.zeros(n_row * n_col)\n",
    "    for i in range(n_row):\n",
    "        curr_label = selected_labels[i]\n",
    "        for j in range(n_col):\n",
    "            y_fixed[i * n_col + j] = curr_label\n",
    "    y_fixed = torch.from_numpy(y_fixed).type(torch.float).reshape(-1, 1).cuda()\n",
    "\n",
    "    # Start timer\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    # Start training\n",
    "    for niter in range(RESUME_NITERS_GAN, NITERS_GAN):\n",
    "\n",
    "        # === Train Distriminator ===\n",
    "\n",
    "        for _ in range(NUM_D_STEPS):\n",
    "\n",
    "            ## randomly draw batch_size_disc y's from unique_train_labels\n",
    "            batch_target_labels_in_dataset = np.random.choice(\n",
    "                unique_train_labels, size=BATCH_SIZE_DISC, replace=True\n",
    "            )\n",
    "            ## add Gaussian noise; we estimate image distribution conditional on these labels\n",
    "            batch_epsilons = np.random.normal(0, kernel_sigma, BATCH_SIZE_DISC)\n",
    "            batch_target_labels = batch_target_labels_in_dataset + batch_epsilons\n",
    "\n",
    "            ## find index of real images with labels in the vicinity of batch_target_labels\n",
    "            ## generate labels for fake image generation; these labels are also in the vicinity of batch_target_labels\n",
    "            batch_real_indx = np.zeros(\n",
    "                BATCH_SIZE_DISC, dtype=int\n",
    "            )  # index of images in the datata; the labels of these images are in the vicinity\n",
    "            batch_fake_labels = np.zeros(BATCH_SIZE_DISC)\n",
    "\n",
    "            for j in range(BATCH_SIZE_DISC):\n",
    "                ## index for real images\n",
    "                if THRESHOLD_TYPE == \"hard\":\n",
    "                    indx_real_in_vicinity = np.where(\n",
    "                        np.abs(train_labels - batch_target_labels[j]) <= kappa\n",
    "                    )[0]\n",
    "                else:\n",
    "                    # reverse the weight function for SVDL\n",
    "                    indx_real_in_vicinity = np.where(\n",
    "                        (train_labels - batch_target_labels[j]) ** 2\n",
    "                        <= -np.log(NONZERO_SOFT_WEIGHT_THRESHOLD) / kappa\n",
    "                    )[0]\n",
    "\n",
    "                ## if the max gap between two consecutive ordered unique labels is large, it is possible that len(indx_real_in_vicinity)<1\n",
    "                while len(indx_real_in_vicinity) < 1:\n",
    "                    batch_epsilons_j = np.random.normal(0, kernel_sigma, 1)\n",
    "                    batch_target_labels[j] = (\n",
    "                        batch_target_labels_in_dataset[j] + batch_epsilons_j\n",
    "                    )\n",
    "                    if clip_label:\n",
    "                        batch_target_labels = np.clip(batch_target_labels, 0.0, 1.0)\n",
    "                    ## index for real images\n",
    "                    if THRESHOLD_TYPE == \"hard\":\n",
    "                        indx_real_in_vicinity = np.where(\n",
    "                            np.abs(train_labels - batch_target_labels[j]) <= kappa\n",
    "                        )[0]\n",
    "                    else:\n",
    "                        # reverse the weight function for SVDL\n",
    "                        indx_real_in_vicinity = np.where(\n",
    "                            (train_labels - batch_target_labels[j]) ** 2\n",
    "                            <= -np.log(NONZERO_SOFT_WEIGHT_THRESHOLD) / kappa\n",
    "                        )[0]\n",
    "                # end while len(indx_real_in_vicinity)<1\n",
    "\n",
    "                assert len(indx_real_in_vicinity) >= 1\n",
    "\n",
    "                batch_real_indx[j] = np.random.choice(indx_real_in_vicinity, size=1)[0]\n",
    "\n",
    "                ## labels for fake images generation\n",
    "                if THRESHOLD_TYPE == \"hard\":\n",
    "                    lb = batch_target_labels[j] - kappa\n",
    "                    ub = batch_target_labels[j] + kappa\n",
    "                else:\n",
    "                    lb = batch_target_labels[j] - np.sqrt(\n",
    "                        -np.log(NONZERO_SOFT_WEIGHT_THRESHOLD) / kappa\n",
    "                    )\n",
    "                    ub = batch_target_labels[j] + np.sqrt(\n",
    "                        -np.log(NONZERO_SOFT_WEIGHT_THRESHOLD) / kappa\n",
    "                    )\n",
    "                lb = max(0.0, lb)\n",
    "                ub = min(ub, 1.0)\n",
    "                assert lb <= ub\n",
    "                assert lb >= 0 and ub >= 0\n",
    "                assert lb <= 1 and ub <= 1\n",
    "                batch_fake_labels[j] = np.random.uniform(lb, ub, size=1)[0]\n",
    "\n",
    "            ## draw real image/label batch from the training set\n",
    "            batch_real_images = torch.from_numpy(\n",
    "                normalize_images(train_images[batch_real_indx])\n",
    "            )\n",
    "            batch_real_images = batch_real_images.type(torch.float).cuda()\n",
    "            batch_real_labels = train_labels[batch_real_indx]\n",
    "            batch_real_labels = (\n",
    "                torch.from_numpy(batch_real_labels).type(torch.float).cuda()\n",
    "            )\n",
    "\n",
    "            ## generate the fake image batch\n",
    "            batch_fake_labels = (\n",
    "                torch.from_numpy(batch_fake_labels).type(torch.float).cuda()\n",
    "            )\n",
    "            z = torch.randn(BATCH_SIZE_DISC, DIM_GAN, dtype=torch.float).cuda()\n",
    "            batch_fake_images = netG(z, net_y2h(batch_fake_labels))\n",
    "\n",
    "            ## target labels on gpu\n",
    "            batch_target_labels = (\n",
    "                torch.from_numpy(batch_target_labels).type(torch.float).cuda()\n",
    "            )\n",
    "\n",
    "            ## weight vector\n",
    "            if THRESHOLD_TYPE == \"soft\":\n",
    "                real_weights = torch.exp(\n",
    "                    -kappa * (batch_real_labels - batch_target_labels) ** 2\n",
    "                ).cuda()\n",
    "                fake_weights = torch.exp(\n",
    "                    -kappa * (batch_fake_labels - batch_target_labels) ** 2\n",
    "                ).cuda()\n",
    "            else:\n",
    "                real_weights = torch.ones(BATCH_SIZE_DISC, dtype=torch.float).cuda()\n",
    "                fake_weights = torch.ones(BATCH_SIZE_DISC, dtype=torch.float).cuda()\n",
    "            # end if threshold type\n",
    "\n",
    "            # forward pass\n",
    "            if GAN_DIFFAUGMENT:\n",
    "                real_dis_out = netD(\n",
    "                    DiffAugment(batch_real_images, policy=GAN_DIFFAUGMENT_POLICY),\n",
    "                    net_y2h(batch_target_labels),\n",
    "                )\n",
    "                fake_dis_out = netD(\n",
    "                    DiffAugment(\n",
    "                        batch_fake_images.detach(), policy=GAN_DIFFAUGMENT_POLICY\n",
    "                    ),\n",
    "                    net_y2h(batch_target_labels),\n",
    "                )\n",
    "            else:\n",
    "                real_dis_out = netD(batch_real_images, net_y2h(batch_target_labels))\n",
    "                fake_dis_out = netD(\n",
    "                    batch_fake_images.detach(), net_y2h(batch_target_labels)\n",
    "                )\n",
    "\n",
    "            if LOSS_TYPE_GAN == \"vanilla\":\n",
    "                real_dis_out = torch.nn.Sigmoid()(real_dis_out)\n",
    "                fake_dis_out = torch.nn.Sigmoid()(fake_dis_out)\n",
    "                d_loss_real = -torch.log(real_dis_out + 1e-20)\n",
    "                d_loss_fake = -torch.log(1 - fake_dis_out + 1e-20)\n",
    "            elif LOSS_TYPE_GAN == \"hinge\":\n",
    "                d_loss_real = torch.nn.ReLU()(1.0 - real_dis_out)\n",
    "                d_loss_fake = torch.nn.ReLU()(1.0 + fake_dis_out)\n",
    "            else:\n",
    "                raise ValueError(\"Not supported loss type!!!\")\n",
    "\n",
    "            d_loss = torch.mean(\n",
    "                real_weights.reshape(-1) * d_loss_real.reshape(-1)\n",
    "            ) + torch.mean(fake_weights.reshape(-1) * d_loss_fake.reshape(-1))\n",
    "\n",
    "            optimizerD.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "\n",
    "        netG.train()\n",
    "\n",
    "        # Choose target labels for fake images\n",
    "        batch_target_labels_in_dataset = np.random.choice(\n",
    "            unique_train_labels, size=BATCH_SIZE_GENE, replace=True\n",
    "        )\n",
    "\n",
    "        # Add Gaussian noise\n",
    "        batch_epsilons = np.random.normal(0, kernel_sigma, BATCH_SIZE_GENE)\n",
    "        batch_target_labels = batch_target_labels_in_dataset + batch_epsilons\n",
    "        batch_target_labels = (\n",
    "            torch.from_numpy(batch_target_labels).type(torch.float).cuda()\n",
    "        )\n",
    "\n",
    "        # Add random noise\n",
    "        z = torch.randn(BATCH_SIZE_GENE, DIM_GAN, dtype=torch.float).cuda()\n",
    "\n",
    "        # Generate fake images\n",
    "        batch_fake_images = netG(z, net_y2h(batch_target_labels))\n",
    "\n",
    "        # Calculate the generator loss\n",
    "        if GAN_DIFFAUGMENT:\n",
    "            dis_out = netD(\n",
    "                DiffAugment(batch_fake_images, policy=GAN_DIFFAUGMENT_POLICY),\n",
    "                net_y2h(batch_target_labels),\n",
    "            )\n",
    "        else:\n",
    "            dis_out = netD(batch_fake_images, net_y2h(batch_target_labels))\n",
    "        if LOSS_TYPE_GAN == \"vanilla\":\n",
    "            dis_out = torch.nn.Sigmoid()(dis_out)\n",
    "            g_loss = -torch.mean(torch.log(dis_out + 1e-20))\n",
    "        elif LOSS_TYPE_GAN == \"hinge\":\n",
    "            g_loss = -dis_out.mean()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizerG.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # === Logging ===\n",
    "\n",
    "        # Every 20 iterations, print the loss\n",
    "        if (niter + 1) % 20 == 0:\n",
    "            new_row = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"niter\": niter + 1,\n",
    "                        \"d_loss\": d_loss.item(),\n",
    "                        \"g_loss\": g_loss.item(),\n",
    "                        \"real_prob\": real_dis_out.mean().item(),\n",
    "                        \"fake_prob\": fake_dis_out.mean().item(),\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            loss_df = pd.concat([loss_df, new_row], ignore_index=True)\n",
    "\n",
    "            print(\n",
    "                \"CcGAN,%s: [Iter %d/%d] [D loss: %.4e] [G loss: %.4e] [real prob: %.3f] [fake prob: %.3f] [Time: %.4f]\"\n",
    "                % (\n",
    "                    GAN_ARCH,\n",
    "                    niter + 1,\n",
    "                    NITERS_GAN,\n",
    "                    d_loss.item(),\n",
    "                    g_loss.item(),\n",
    "                    real_dis_out.mean().item(),\n",
    "                    fake_dis_out.mean().item(),\n",
    "                    timeit.default_timer() - start_time,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Every `VISUALIZE_FREQ` iterations, visualize the generated images\n",
    "        if (niter + 1) % VISUALIZE_FREQ == 0:\n",
    "            netG.eval()\n",
    "            with torch.no_grad():\n",
    "                gen_imgs = netG(z_fixed, net_y2h(y_fixed))\n",
    "                gen_imgs = gen_imgs.detach().cpu()\n",
    "                save_image(\n",
    "                    gen_imgs.data,\n",
    "                    os.path.join(SAVE_IMAGES_DIR, \"{}.png\".format(niter + 1)),\n",
    "                    nrow=n_row,\n",
    "                    normalize=True,\n",
    "                )\n",
    "\n",
    "        # Every `SAVE_NITERS_FREQ` iterations, save the model\n",
    "        if CCGAN_MODELS_DIR is not None and (\n",
    "            (niter + 1) % SAVE_NITERS_FREQ == 0 or (niter + 1) == NITERS_GAN\n",
    "        ):\n",
    "            save_file = os.path.join(\n",
    "                CCGAN_MODELS_DIR,\n",
    "                \"CcGAN_{}_nDsteps_{}\".format(THRESHOLD_TYPE, NUM_D_STEPS),\n",
    "                \"ckpt_CcGAN_niters_{}.pth\".format(niter + 1),\n",
    "            )\n",
    "\n",
    "            os.makedirs(os.path.dirname(save_file), exist_ok=True)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"netG_state_dict\": netG.state_dict(),\n",
    "                    \"netD_state_dict\": netD.state_dict(),\n",
    "                    \"optimizerG_state_dict\": optimizerG.state_dict(),\n",
    "                    \"optimizerD_state_dict\": optimizerD.state_dict(),\n",
    "                    \"rng_state\": torch.get_rng_state(),\n",
    "                },\n",
    "                save_file,\n",
    "            )\n",
    "\n",
    "    # Save the training loss dataframe\n",
    "    loss_df.to_csv(\n",
    "        os.path.join(\n",
    "            SAVE_OUTPUTS_DIR,\n",
    "            \"CcGAN_{}_nDsteps_{}_loss.csv\".format(THRESHOLD_TYPE, NUM_D_STEPS),\n",
    "        ),\n",
    "        index=False,\n",
    "    )\n",
    "\n",
    "    # Return the trained networks\n",
    "    return netG, netD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3977967",
   "metadata": {},
   "source": [
    "Get the `save_images_in_train_folder` dir and output some basic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf6568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:57.838958Z",
     "iopub.status.busy": "2025-06-11T23:17:57.838695Z",
     "iopub.status.idle": "2025-06-11T23:17:57.841768Z",
     "shell.execute_reply": "2025-06-11T23:17:57.841349Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CcGAN: {}, {}, Sigma is {}, Kappa is {}.\".format(GAN_ARCH, THRESHOLD_TYPE, KERNEL_SIGMA, KAPPA))\n",
    "save_images_in_train_folder = os.path.join(SAVE_IMAGES_DIR, \"{}_{:.3f}_{:.3f}_in_train\".format(THRESHOLD_TYPE, KERNEL_SIGMA, KAPPA))\n",
    "os.makedirs(save_images_in_train_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a26253a",
   "metadata": {},
   "source": [
    "Start the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7db4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:57.843551Z",
     "iopub.status.busy": "2025-06-11T23:17:57.843298Z",
     "iopub.status.idle": "2025-06-11T23:17:57.845883Z",
     "shell.execute_reply": "2025-06-11T23:17:57.845462Z"
    }
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "print(\"Begin Training: %s\" % GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9c4ef",
   "metadata": {},
   "source": [
    "The following code is used for training CcGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4284b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T23:17:57.847694Z",
     "iopub.status.busy": "2025-06-11T23:17:57.847448Z",
     "iopub.status.idle": "2025-06-12T08:21:51.285654Z",
     "shell.execute_reply": "2025-06-12T08:21:51.285142Z"
    }
   },
   "outputs": [],
   "source": [
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}\n",
    "\n",
    "def DiffAugment(x, policy='', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "Filename_GAN = os.path.join(CCGAN_MODELS_DIR, 'ckpt_CcGAN_niters_{}_nDsteps_{}_{}_{:.3f}_{:.3f}_seed_{}.pth'.format(NITERS_GAN, NUM_D_STEPS, THRESHOLD_TYPE, KERNEL_SIGMA, KAPPA, SEED))\n",
    "\n",
    "if not os.path.isfile(Filename_GAN):\n",
    "    netG = CcGAN_SAGAN_Generator(dim_z=DIM_GAN, dim_embed=DIM_EMBED)\n",
    "    netD = CcGAN_SAGAN_Discriminator(dim_embed=DIM_EMBED)\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netD = nn.DataParallel(netD)\n",
    "\n",
    "    # Start training\n",
    "    netG, netD = train_ccgan(KERNEL_SIGMA, KAPPA, images_train, labels_train, netG, netD, net_y2h, SAVE_IMAGES_DIR=save_images_in_train_folder, CCGAN_MODELS_DIR = CCGAN_MODELS_DIR)\n",
    "\n",
    "    # Store model\n",
    "    torch.save({\n",
    "        'netG_state_dict': netG.state_dict(),\n",
    "    }, Filename_GAN)\n",
    "\n",
    "else:\n",
    "    print(\"Loading pre-trained generator.\")\n",
    "    checkpoint = torch.load(Filename_GAN)\n",
    "    netG = CcGAN_SAGAN_Generator(dim_z=DIM_GAN, dim_embed=DIM_EMBED).cuda()\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netG.load_state_dict(checkpoint['netG_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31391f1b",
   "metadata": {},
   "source": [
    "End the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb947260",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T08:21:51.288152Z",
     "iopub.status.busy": "2025-06-12T08:21:51.287841Z",
     "iopub.status.idle": "2025-06-12T08:21:51.290704Z",
     "shell.execute_reply": "2025-06-12T08:21:51.290212Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = timeit.default_timer()\n",
    "print(\"GAN training finished; Time elapses: {}s\".format(stop - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
