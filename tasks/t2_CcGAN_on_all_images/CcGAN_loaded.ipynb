{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dfc621",
   "metadata": {},
   "source": [
    "# CcGAN Loaded\n",
    "\n",
    "This notebook is used for loading CcGAN and generating images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d1100",
   "metadata": {},
   "source": [
    "## Step 1 - Import and Settings\n",
    "\n",
    "Import others' libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e679105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380a083",
   "metadata": {},
   "source": [
    "Import our own libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc0e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7071ff",
   "metadata": {},
   "source": [
    "Set hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300d52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Settings\n",
    "DATA_PATH = \"./datasets/Ra_128_indexed.h5\"\n",
    "SAVE_OUTPUTS_DIR = \"./output/saved_outputs\"\n",
    "SAVE_IMAGES_DIR = \"./output/saved_images\"\n",
    "GENERATE_IMAGES_DIR = \"./output/generated_images\"\n",
    "EMBED_MODELS_DIR = \"./output/embed_models\"\n",
    "CCGAN_MODELS_DIR = \"./output/CcGAN_models\"\n",
    "SEED = 42\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# Dataset\n",
    "DATA_SPLIT = \"train\"\n",
    "MIN_LABEL = 1.3\n",
    "MAX_LABEL = 5.2\n",
    "NUM_CHANNELS = 3\n",
    "IMG_SIZE = 128\n",
    "MAX_NUM_IMG_PER_LABEL = 1000\n",
    "MAX_NUM_IMG_PER_LABEL_AFTER_REPLICA = 0\n",
    "SHOW_REAL_IMGS = True\n",
    "VISUALIZE_FAKE_IMAGES = True\n",
    "\n",
    "# Embedding Settings\n",
    "NET_EMBED = \"ResNet34_embed\"\n",
    "DIM_EMBED = 128\n",
    "\n",
    "# Embedding Training Settings\n",
    "BASE_LR_X2Y = 0.01\n",
    "BASE_LR_Y2H = 0.01\n",
    "EPOCH_NET_EMBED = 200\n",
    "RESUME_EPOCH_NET_EMBED = 0\n",
    "EPOCH_NET_Y2H = 500\n",
    "BATCH_SIZE_EMBED = 256\n",
    "\n",
    "# GAN Settings\n",
    "GAN = \"CcGAN\"\n",
    "GAN_ARCH = \"SAGAN\"\n",
    "LOSS_TYPE_GAN = \"hinge\"\n",
    "DIM_GAN = 128\n",
    "CGAN_NUM_CLASSES = 20\n",
    "KERNEL_SIGMA = 0.04697700151079382\n",
    "THRESHOLD_TYPE = \"soft\"\n",
    "KAPPA = 108.73916897823854\n",
    "\n",
    "# GAN Training Settings\n",
    "NITERS_GAN = 30000\n",
    "RESUME_NITERS_GAN = 0\n",
    "SAVE_NITERS_FREQ = 5000\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "BATCH_SIZE_DISC = 64\n",
    "BATCH_SIZE_GENE = 64\n",
    "NUM_D_STEPS = 4\n",
    "VISUALIZE_FREQ = 1000\n",
    "NONZERO_SOFT_WEIGHT_THRESHOLD = 1e-3\n",
    "\n",
    "# DiffAugment Settings\n",
    "GAN_DIFFAUGMENT = True\n",
    "GAN_DIFFAUGMENT_POLICY = \"color,translation,cutout\"\n",
    "\n",
    "# Evaluation Settings\n",
    "EVAL_MODE = 2\n",
    "NUM_EVAL_LABELS = -1\n",
    "SAMP_BATCH_SIZE = 1000\n",
    "NFAKE_PER_LABEL = 200\n",
    "NREAL_PER_LABEL = -1\n",
    "COMP_FID = True\n",
    "EPOCH_FID_CNN = 200\n",
    "FID_RADIUS = 0\n",
    "FID_NUM_CENTERS = -1\n",
    "DUMP_FAKE_FOR_NIQE = True\n",
    "\n",
    "# Generate Settings\n",
    "NUM_GENERATE = 100\n",
    "BATCH_SIZE_FOR_GENERATE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf3110",
   "metadata": {},
   "source": [
    "Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97912ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Seeds\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths - From arguments\n",
    "if not os.path.exists(SAVE_OUTPUTS_DIR):\n",
    "    os.makedirs(SAVE_OUTPUTS_DIR, exist_ok=True)\n",
    "if not os.path.exists(SAVE_IMAGES_DIR):\n",
    "    os.makedirs(SAVE_IMAGES_DIR, exist_ok=True)\n",
    "if not os.path.exists(GENERATE_IMAGES_DIR):\n",
    "    os.makedirs(GENERATE_IMAGES_DIR, exist_ok=True)\n",
    "if not os.path.exists(EMBED_MODELS_DIR):\n",
    "    os.makedirs(EMBED_MODELS_DIR, exist_ok=True)\n",
    "if not os.path.exists(CCGAN_MODELS_DIR):\n",
    "    os.makedirs(CCGAN_MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# Paths - Embedding\n",
    "EMBED_X2Y_CKPT_DIR = os.path.join(\n",
    "    EMBED_MODELS_DIR,\n",
    "    f\"ckpt_embed_x2y_{NET_EMBED}_dim_{DIM_EMBED}_batchSize_{BATCH_SIZE_EMBED}_lr_{BASE_LR_X2Y:.0e}_seed_{SEED}\",\n",
    ")\n",
    "if not os.path.exists(EMBED_X2Y_CKPT_DIR):\n",
    "    os.makedirs(EMBED_X2Y_CKPT_DIR)\n",
    "EMBED_X2Y_PATH = os.path.join(\n",
    "    EMBED_MODELS_DIR,\n",
    "    f\"embed_x2y_{NET_EMBED}_dim_{DIM_EMBED}_batchSize_{BATCH_SIZE_EMBED}_lr_{BASE_LR_X2Y:.0e}_epoch_{EPOCH_NET_EMBED}_seed_{SEED}.pth\",\n",
    ")\n",
    "Y2H_PATH = os.path.join(\n",
    "    EMBED_MODELS_DIR,\n",
    "    f\"y2h_{NET_EMBED}_dim_{DIM_EMBED}_batchSize_{BATCH_SIZE_EMBED}_lr_{BASE_LR_Y2H:.0e}_epoch_{EPOCH_NET_Y2H}_seed_{SEED}.pth\",\n",
    ")\n",
    "\n",
    "# Paths - CcGAN\n",
    "CCGAN_CKPT_DIR = os.path.join(\n",
    "    CCGAN_MODELS_DIR,\n",
    "    f\"ckpt_CcGAN_{GAN_ARCH}_dim_{DIM_GAN}_{IMG_SIZE}_batchSizeG_{BATCH_SIZE_GENE}_batchSizeD_{BATCH_SIZE_DISC}_lrG_{LR_G:.0e}_lrD_{LR_D:.0e}_nDsteps_{NUM_D_STEPS}_{THRESHOLD_TYPE}_{KERNEL_SIGMA:.3f}_{KAPPA:.3f}_loss_{LOSS_TYPE_GAN}_seed_{SEED}\",\n",
    ")\n",
    "if not os.path.exists(CCGAN_CKPT_DIR):\n",
    "    os.makedirs(CCGAN_CKPT_DIR)\n",
    "CCGAN_PATH = os.path.join(\n",
    "    CCGAN_MODELS_DIR,\n",
    "    f\"CcGAN_{GAN_ARCH}_dim_{DIM_GAN}_{IMG_SIZE}_batchSizeG_{BATCH_SIZE_GENE}_batchSizeD_{BATCH_SIZE_DISC}_lrG_{LR_G:.0e}_lrD_{LR_D:.0e}_nIters_{NITERS_GAN}_nDsteps_{NUM_D_STEPS}_{THRESHOLD_TYPE}_{KERNEL_SIGMA:.3f}_{KAPPA:.3f}_loss_{LOSS_TYPE_GAN}_seed_{SEED}.pth\",\n",
    ")\n",
    "IMAGES_IN_TRAIN_DIR = os.path.join(\n",
    "    SAVE_IMAGES_DIR,\n",
    "    f\"CcGAN_{GAN_ARCH}_dim_{DIM_GAN}_{IMG_SIZE}_batchSizeG_{BATCH_SIZE_GENE}_batchSizeD_{BATCH_SIZE_DISC}_lrG_{LR_G:.0e}_lrD_{LR_D:.0e}_nDsteps_{NUM_D_STEPS}_{THRESHOLD_TYPE}_{KERNEL_SIGMA:.3f}_{KAPPA:.3f}_loss_{LOSS_TYPE_GAN}_seed_{SEED}_in_train\",\n",
    ")\n",
    "if not os.path.exists(IMAGES_IN_TRAIN_DIR):\n",
    "    os.makedirs(IMAGES_IN_TRAIN_DIR)\n",
    "OUTPUT_PATH = os.path.join(\n",
    "    SAVE_OUTPUTS_DIR,\n",
    "    f\"CcGAN_{GAN_ARCH}_dim_{DIM_GAN}_{IMG_SIZE}_batchSizeG_{BATCH_SIZE_GENE}_batchSizeD_{BATCH_SIZE_DISC}_lrG_{LR_G:.0e}_lrD_{LR_D:.0e}_nIters_{NITERS_GAN}_nDsteps_{NUM_D_STEPS}_{THRESHOLD_TYPE}_{KERNEL_SIGMA:.3f}_{KAPPA:.3f}_loss_{LOSS_TYPE_GAN}_seed_{SEED}_loss.csv\",\n",
    ")\n",
    "IMAGES_IN_GENERATE_DIR = os.path.join(\n",
    "    GENERATE_IMAGES_DIR,\n",
    "    f\"CcGAN_{GAN_ARCH}_dim_{DIM_GAN}_{IMG_SIZE}_batchSizeG_{BATCH_SIZE_GENE}_batchSizeD_{BATCH_SIZE_DISC}_lrG_{LR_G:.0e}_lrD_{LR_D:.0e}_nDsteps_{NUM_D_STEPS}_{THRESHOLD_TYPE}_{KERNEL_SIGMA:.3f}_{KAPPA:.3f}_loss_{LOSS_TYPE_GAN}_seed_{SEED}_in_generate\",\n",
    ")\n",
    "if not os.path.exists(IMAGES_IN_GENERATE_DIR):\n",
    "    os.makedirs(IMAGES_IN_GENERATE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e2965",
   "metadata": {},
   "source": [
    "## Step 2 - Load Pretrained Embedding Models\n",
    "\n",
    "In this step we load `net_embed` and `net_y2h` models to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b535f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): model_y2h(\n",
       "    (main): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (1): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (8): ReLU()\n",
       "      (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (10): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "      (11): ReLU()\n",
       "      (12): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (13): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if NET_EMBED == \"ResNet18_embed\":\n",
    "    net_embed = ResNet18_embed(dim_embed=DIM_EMBED)\n",
    "elif NET_EMBED == \"ResNet34_embed\":\n",
    "    net_embed = ResNet34_embed(dim_embed=DIM_EMBED)\n",
    "elif NET_EMBED == \"ResNet50_embed\":\n",
    "    net_embed = ResNet50_embed(dim_embed=DIM_EMBED)\n",
    "net_embed = net_embed.to(device)\n",
    "net_embed = nn.DataParallel(net_embed)\n",
    "\n",
    "net_y2h = model_y2h(dim_embed=DIM_EMBED)\n",
    "net_y2h = net_y2h.to(device)\n",
    "net_y2h = nn.DataParallel(net_y2h)\n",
    "\n",
    "## (1). Load net_embed first: x2h+h2y\n",
    "checkpoint = torch.load(EMBED_X2Y_PATH, map_location=device)\n",
    "net_embed.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "\n",
    "## (2). Load y2h\n",
    "checkpoint = torch.load(Y2H_PATH, map_location=device)\n",
    "net_y2h.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "\n",
    "net_embed.eval()\n",
    "net_h2y = net_embed.module.h2y\n",
    "net_y2h.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d59c7",
   "metadata": {},
   "source": [
    "## Step 3 - Load CcGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b022ac8",
   "metadata": {},
   "source": [
    "This step loads `netG` from `.pth` file, and switch it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4284b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CcGAN_SAGAN_Generator(\n",
       "    (snlinear0): Linear(in_features=128, out_features=16384, bias=True)\n",
       "    (block1): GenBlock(\n",
       "      (cond_bn1): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=1024, bias=False)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (snconv2d1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cond_bn2): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=1024, bias=False)\n",
       "      )\n",
       "      (snconv2d2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (snconv2d0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (block2): GenBlock(\n",
       "      (cond_bn1): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=1024, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=1024, bias=False)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (snconv2d1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cond_bn2): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=512, bias=False)\n",
       "      )\n",
       "      (snconv2d2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (snconv2d0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (block3): GenBlock(\n",
       "      (cond_bn1): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=512, bias=False)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (snconv2d1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cond_bn2): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=256, bias=False)\n",
       "      )\n",
       "      (snconv2d2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (snconv2d0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (self_attn): Self_Attn(\n",
       "      (snconv1x1_theta): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (snconv1x1_phi): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (snconv1x1_g): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (snconv1x1_attn): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (block4): GenBlock(\n",
       "      (cond_bn1): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=256, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=256, bias=False)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (snconv2d1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cond_bn2): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (snconv2d2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (snconv2d0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (block5): GenBlock(\n",
       "      (cond_bn1): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (snconv2d1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cond_bn2): ConditionalBatchNorm2d(\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=False, track_running_stats=True)\n",
       "        (embed_gamma): Linear(in_features=128, out_features=64, bias=False)\n",
       "        (embed_beta): Linear(in_features=128, out_features=64, bias=False)\n",
       "      )\n",
       "      (snconv2d2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (snconv2d0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.0001, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (snconv2d1): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(CCGAN_PATH, map_location=device)\n",
    "netG = CcGAN_SAGAN_Generator(dim_z=DIM_GAN, dim_embed=DIM_EMBED).to(device)\n",
    "netG = nn.DataParallel(netG)\n",
    "netG.load_state_dict(checkpoint[\"netG_state_dict\"])\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc475dd0",
   "metadata": {},
   "source": [
    "## Step 4 - Generate Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca6825",
   "metadata": {},
   "source": [
    "Define a function to generate a batch of images `generate_batch_images`.\n",
    "\n",
    "Define a function to generate all images `generate_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32948a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_images(labels, start_index, batch_size):\n",
    "\n",
    "    # Labels to generate\n",
    "    if start_index + batch_size > labels.shape[0]:\n",
    "        labels_to_generate_raw = np.array(labels[start_index:])\n",
    "    else:\n",
    "        labels_to_generate_raw = np.array(\n",
    "            labels[start_index : (start_index + batch_size)]\n",
    "        )\n",
    "\n",
    "    # Calculate\n",
    "    labels_to_generate = (labels_to_generate_raw - MIN_LABEL) / (MAX_LABEL - MIN_LABEL)\n",
    "    z = torch.randn(labels_to_generate.shape[0], IMG_SIZE, dtype=torch.float).to(device)\n",
    "    y = torch.from_numpy(labels_to_generate).type(torch.float).reshape(-1, 1).to(device)\n",
    "\n",
    "    # Generate and save fake images\n",
    "    batch_fake_images = netG(z, net_y2h(y))\n",
    "    batch_fake_images = batch_fake_images.detach().cpu()\n",
    "    for j in range(batch_fake_images.shape[0]):\n",
    "        fake_image = batch_fake_images[j].unsqueeze(0)\n",
    "        save_image(\n",
    "            fake_image.data,\n",
    "            os.path.join(\n",
    "                IMAGES_IN_GENERATE_DIR,\n",
    "                f\"{start_index+j+1}_{labels_to_generate_raw[j]:.3f}.png\",\n",
    "            ),\n",
    "            nrow=1,\n",
    "            normalize=True,\n",
    "        )\n",
    "    print(\n",
    "        f\"Generated {len(batch_fake_images)} images (label range: [{labels_to_generate_raw[0]:.3f}, {labels_to_generate_raw[-1]:.3f}]) and saved to {IMAGES_IN_GENERATE_DIR}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_images(labels):\n",
    "    start_index = 0\n",
    "    while start_index < labels.shape[0]:\n",
    "        generate_batch_images(labels, start_index, BATCH_SIZE_FOR_GENERATE)\n",
    "        start_index += BATCH_SIZE_FOR_GENERATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2b7b1",
   "metadata": {},
   "source": [
    "Generate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d5f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 16 images (label range: [1.300, 1.891]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n",
      "Generated 16 images (label range: [1.930, 2.521]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n",
      "Generated 16 images (label range: [2.561, 3.152]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n",
      "Generated 16 images (label range: [3.191, 3.782]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n",
      "Generated 16 images (label range: [3.821, 4.412]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n",
      "Generated 16 images (label range: [4.452, 5.042]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n",
      "Generated 4 images (label range: [5.082, 5.200]) and saved to ./output/generated_images/CcGAN_SAGAN_dim_128_128_batchSizeG_64_batchSizeD_64_lrG_1e-04_lrD_1e-04_nDsteps_4_soft_0.047_108.739_loss_hinge_seed_42_in_generate\n"
     ]
    }
   ],
   "source": [
    "labels = np.linspace(MIN_LABEL, MAX_LABEL, num=100).astype(float)\n",
    "generate_images(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025-04-23_macOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
